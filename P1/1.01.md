### 1. Una fuente de información binaria con memoria nula produce el símbolo $s_0$ con probabilidad $p_0$ y el símbolo $s_1$ con probabilidad $p_1 = 1 − p_0$.

### a) Formular la entropía de la fuente como función de $p_0$.

Tenemos
$$H(p_0) = p_0 I(s_0) + p_1I(s_1) = p_0\log\frac{1}{p_0} + p_1 \log \frac{1}{p_1}$$

### b) Graficar $H(p_0)$.

Ver en Information Theory and Coding (p. 19).

### c) Dar una interpretación de los puntos de la gráfica que considere interesantes.

Son de interés $p_0 = 1/2$ donde la función tiene un máximo global y $p_0 = 0, p_0 = 1$, donde la función se indefine pero tiende a $0$.

En el primer caso, vemos que la entropía es máxima cuando todos los símbolos son equiprobables. En los otros, que si un evento siempre ocurre ($p_0$ o $p_1$), entonces su comunicación no conlleva información.
